{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b090e3",
   "metadata": {
    "executionInfo": {
     "elapsed": 36644,
     "status": "ok",
     "timestamp": 1681581829714,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "Ow8d3ImlSfI_",
    "outputId": "5cba6e8a-145f-4472-9e45-1cff852728da",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-04-18T04:55:36.554697",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/datasets/prakharrathi25/google-play-store-reviews\n",
    "#https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/\n",
    "!pip -q install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0eed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T01:48:09.517238Z",
     "iopub.status.busy": "2023-04-18T01:48:09.516629Z",
     "iopub.status.idle": "2023-04-18T01:48:11.592081Z",
     "shell.execute_reply": "2023-04-18T01:48:11.590979Z",
     "shell.execute_reply.started": "2023-04-18T01:48:09.517191Z"
    },
    "executionInfo": {
     "elapsed": 26937,
     "status": "ok",
     "timestamp": 1681581862342,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "ByrHQCFaSbJa",
    "outputId": "c50450ae-1fce-41e0-8b7d-0019c843c5ef",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as opt\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, auc, roc_curve\n",
    "from copy import copy, deepcopy\n",
    "import zipfile\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # specify which GPU(s) to be used\n",
    "#torch.backends.cudnn.benchmark = True'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5024aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T01:49:19.149207Z",
     "iopub.status.busy": "2023-04-18T01:49:19.148077Z",
     "iopub.status.idle": "2023-04-18T01:49:19.267073Z",
     "shell.execute_reply": "2023-04-18T01:49:19.265933Z",
     "shell.execute_reply.started": "2023-04-18T01:49:19.149154Z"
    },
    "executionInfo": {
     "elapsed": 3332,
     "status": "ok",
     "timestamp": 1681581866865,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "-TDb_CRISVOI",
    "outputId": "76c2cadc-958d-4d09-bcc4-87f3fbba0946",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/app-reviews/reviews.csv')\n",
    "def to_sentiment(rating):\n",
    "  rating = int(rating)\n",
    "  if rating <= 2:\n",
    "    return 'negative'\n",
    "  elif rating == 3:\n",
    "    return 'neutral'\n",
    "  else: \n",
    "    return 'positive'\n",
    "\n",
    "df['sentiment'] = df.score.apply(to_sentiment)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f9d84",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb88e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'Dtype': df.dtypes, 'Nunique': df.nunique(), 'Isnull': df.isnull().sum()}, index=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df05b32",
   "metadata": {
    "id": "cVN2_BwXSnag",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = [len(sent) for sent in df['content']]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(df)), y=seq_len, mode='markers', name='Seq len'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(df)), y=[np.mean(seq_len)]*len(seq_len), mode='lines', name='Avg seq len'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(df)), y=[np.median(seq_len)]*len(seq_len), mode='lines', name='Med seq len'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0cb09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T01:49:26.303993Z",
     "iopub.status.busy": "2023-04-18T01:49:26.303376Z",
     "iopub.status.idle": "2023-04-18T01:49:26.334325Z",
     "shell.execute_reply": "2023-04-18T01:49:26.333195Z",
     "shell.execute_reply.started": "2023-04-18T01:49:26.303953Z"
    },
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1681581870893,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "4nb2vF2EC13N",
    "outputId": "01c545f2-673c-476a-8aaf-c04451efe803",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''example_text = ['I will watch Memento tonight']\n",
    "bert_input = tokenizer(example_text,padding='max_length', max_length = 10, truncation=True, return_tensors=\"pt\")\n",
    "tokenizer.decode(bert_input.input_ids[1])'''\n",
    "text_column, out_column = 'content', 'sentiment'\n",
    "df = df[[text_column, out_column]]\n",
    "labels = dict(zip(df[out_column].unique(), range(df[out_column].nunique())))\n",
    "df.replace({out_column: labels}, inplace=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df[text_column], df[out_column], test_size=0.1, random_state=42)\n",
    "X_valid, X_test, Y_valid, Y_test = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)\n",
    "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e532a",
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1681581873399,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "k27YMJY56ksb",
    "outputId": "b7068c62-040d-4a8c-e41c-4af9ad8fdc0e",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af5fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T01:49:44.675582Z",
     "iopub.status.busy": "2023-04-18T01:49:44.675177Z",
     "iopub.status.idle": "2023-04-18T01:49:48.700432Z",
     "shell.execute_reply": "2023-04-18T01:49:48.699396Z",
     "shell.execute_reply.started": "2023-04-18T01:49:44.675544Z"
    },
    "executionInfo": {
     "elapsed": 14291,
     "status": "ok",
     "timestamp": 1681581892106,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "Cj_Yd5UtSpFg",
    "outputId": "453a4983-250c-48ee-ac38-c47da0e171d7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model_config = AutoConfig.from_pretrained(model_path)\n",
    "deberta = AutoModel.from_pretrained(model_path)\n",
    "model_config, deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fb76b",
   "metadata": {
    "id": "PB8171TiqNJD",
    "outputId": "9024ff35-4403-4d03-e3d4-fa69c8388d21",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df[text_column].values[2777]), df[text_column].values[2777]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077044ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T01:50:14.566046Z",
     "iopub.status.busy": "2023-04-18T01:50:14.565224Z",
     "iopub.status.idle": "2023-04-18T01:50:14.584535Z",
     "shell.execute_reply": "2023-04-18T01:50:14.583581Z",
     "shell.execute_reply.started": "2023-04-18T01:50:14.565994Z"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1681581899385,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "KbykiD5mqNJE",
    "outputId": "49d77ca6-e6bf-4fb3-b86e-c8bbe458949c",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = tokenizer([df[text_column].values[2777]], add_special_tokens=True, max_length=512, padding = 'max_length', truncation=True, return_tensors='pt')\n",
    "tmp['input_ids'].shape,tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b6ec6",
   "metadata": {
    "executionInfo": {
     "elapsed": 4026,
     "status": "ok",
     "timestamp": 1681581264688,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "A84yRfMDqNJG",
    "outputId": "46418cf6-1034-4de1-df9e-6ba2aefe3507",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(tmp['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80cf0d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T01:50:20.453341Z",
     "iopub.status.busy": "2023-04-18T01:50:20.452957Z",
     "iopub.status.idle": "2023-04-18T01:50:20.526775Z",
     "shell.execute_reply": "2023-04-18T01:50:20.525651Z",
     "shell.execute_reply.started": "2023-04-18T01:50:20.453306Z"
    },
    "id": "-9SHWS_lSr0R",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SeqClsLoader(Dataset): #Sequence classification dataloader\n",
    "    def __init__(self, encoded_data, labels):\n",
    "        self.labels = labels\n",
    "        self.encoded_data = encoded_data\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data['input_ids'])\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data['input_ids'][idx], self.encoded_data['attention_mask'][idx], self.labels[idx] if self.labels is not None else None\n",
    "\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, input_shape, units=None, factors=None, activ=True, norm=False, dropout=False, slops=None):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.units = units\n",
    "        self.factors = factors\n",
    "        self.activ, self.norm = activ, norm\n",
    "        self.network = nn.ModuleList()\n",
    "        if self.factors:\n",
    "            self.units = np.round(self.input_shape * np.asarray(self.factors)).astype(int)\n",
    "        if self.units is not None:\n",
    "            self.dropout = np.zeros_like(self.units) if not dropout else dropout\n",
    "            self.slops = np.full(len(self.units), 1) if slops is None else slops\n",
    "            for i, j, k in zip(self.units, self.dropout, self.slops):\n",
    "                if i >= 1:\n",
    "                    block = self.__build_block__(input_shape, i, p=j, slop=k)\n",
    "                    self.network.extend(block)\n",
    "                    input_shape = i\n",
    "        self.output_shape = input_shape\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def __build_block__(self, input_shape, units, p, slop):\n",
    "        block = []\n",
    "        block.append(nn.Linear(input_shape, units, bias=not self.norm))\n",
    "        if self.norm:\n",
    "            block.append(nn.BatchNorm1d(units))\n",
    "            #block.append(nn.LayerNorm(units, eps=1e-5))\n",
    "        if self.activ:\n",
    "            #block.append(nn.LeakyReLU())\n",
    "            block.append(nn.ELU(slop))\n",
    "            #block.append(nn.GELU())\n",
    "        if p > 0:\n",
    "            block.append(nn.Dropout(p))\n",
    "        return block\n",
    " \n",
    "    def forward(self, x):\n",
    "        for layer in self.network:\n",
    "          tmp = layer(x)\n",
    "          x = tmp\n",
    "        return x\n",
    " \n",
    "    def reset_parameters(self):\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                layer.bias.data.fill_(0.1)\n",
    " \n",
    "\n",
    "class TransSeqClassifier(nn.Module):\n",
    "    def __init__(self, model, emb_dim, mlp_units, mlp_dropout, nb_class):\n",
    "        super(TransSeqClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.mlp = NNModel(emb_dim, units=mlp_units, factors=None, dropout=[mlp_dropout]*len(mlp_units)) if mlp_units is not None else None\n",
    "        cls_units = self.mlp.output_shape if mlp_units is not None else emb_dim\n",
    "        self.classifier = nn.Linear(cls_units, nb_class)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        discriminator_hidden_states = self.model(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        last_hidden_state = discriminator_hidden_states[0]\n",
    "        cls_token = last_hidden_state[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        z = self.mlp(cls_token) if self.mlp is not None else cls_token\n",
    "        pred = self.classifier(z)\n",
    "        return z, cls_token, pred\n",
    "\n",
    "class BaseSeqClassifier:\n",
    "    def __init__(self, model, tokenizer, max_len):\n",
    "        self.model = model.to(device)\n",
    "        self.losses = {'Epoch': [], 'Train': [], 'Test': [], 'BState': [], 'LState': [], 'LR': []}\n",
    "        self.tokenizer, self.max_len = tokenizer, max_len\n",
    " \n",
    "    def train_model(self, optim, train_loader, grad_clip, l2_reg):\n",
    "          total_loss = 0\n",
    "          self.model = self.model.train()\n",
    "        #with autograd.detect_anomaly():\n",
    "          for i, (ids, mask, Y) in enumerate(train_loader):\n",
    "              ids, mask, Y = ids.to(device), mask.to(device), Y.to(device)\n",
    "              #self.model.get_weight()\n",
    "              optim.zero_grad()\n",
    "              loss = self.loss_function(ids, mask, Y, l2_reg)\n",
    "              loss.backward()\n",
    "              torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
    "              optim.step()\n",
    "              total_loss += loss.item()\n",
    "          return total_loss/(i+1)\n",
    "        \n",
    " \n",
    "    def eval_model(self, test_loader):\n",
    "        self.model = self.model.eval()\n",
    "        total_loss = 0\n",
    "        for i, (ids, mask, Y) in enumerate(test_loader):\n",
    "            ids, mask, Y = ids.to(device), mask.to(device), Y.to(device)\n",
    "            loss = self.loss_function(ids, mask, Y, l2_reg=0)\n",
    "            total_loss += loss.item()\n",
    "        return total_loss/(i+1)#np.abs(-100. - total_loss)\n",
    " \n",
    "    def fit(self, X_train, Y_train, epoch, lr, opt_kwarg, batch_size=None,  grad_clip=100, momentum=0.9, X_test=None, Y_test=None, l2_reg=0, verbose=True, save=True):\n",
    "        batch_size = len(Y_train) if batch_size is None else batch_size\n",
    "        encoded_train = self.tokenizer(X_train, add_special_tokens=True, max_length=self.max_len, padding = 'max_length', truncation=True, return_tensors='pt')\n",
    "        train_load = DataLoader(SeqClsLoader(encoded_train, Y_train), batch_size=batch_size, shuffle=True)  # DATALOADER obj\n",
    "        if X_test is not None:\n",
    "            encoded_test = self.tokenizer(X_test, add_special_tokens=True, max_length=self.max_len, padding = 'max_length', truncation=True, return_tensors='pt')\n",
    "            test_load = DataLoader(SeqClsLoader(encoded_test, Y_test), batch_size=batch_size, shuffle=True)\n",
    " \n",
    "        best_loss = 1e100\n",
    "        #optim = opt.Adam(self.model.parameters(), lr=lr)\n",
    "        optim = opt.AdamW(self.model.parameters(), lr=lr)\n",
    "        #optim = opt.SGD(self.model.parameters(), lr=lr, momentum=momentum, nesterov=True)\n",
    "\n",
    "        scheduler = None\n",
    "        #scheduler = opt.lr_scheduler.CyclicLR(optim, **opt_kwarg)\n",
    "        #scheduler = opt.lr_scheduler.ReduceLROnPlateau(optim, **opt_kwarg)\n",
    "        #scheduler = opt.lr_scheduler.MultiStepLR(optim, milestones=[28, 120], gamma=0.1)\n",
    "\n",
    "        eval_score = ''\n",
    "        for i in range(epoch):\n",
    "            if verbose:\n",
    "                print('##### EPOCH ' + str(i) + ' #####')\n",
    "               \n",
    "            train_loss = self.train_model(optim, train_load, grad_clip, l2_reg)\n",
    "            self.losses['LState'] = deepcopy(self.model.state_dict())\n",
    "    \n",
    "            if verbose:\n",
    "                print('train loss : ', train_loss)\n",
    "            self.losses['Epoch'].append(i), self.losses['Train'].append(train_loss)\n",
    "    \n",
    "            if X_test is not None:\n",
    "                valid_loss = self.eval_model(test_load)\n",
    "\n",
    "                if verbose:\n",
    "                    print('test loss : ', valid_loss)\n",
    "                self.losses['Test'].append(valid_loss)\n",
    "    \n",
    "                if scheduler is not None:\n",
    "                    scheduler.step(valid_loss)\n",
    "                    self.losses['LR'].append(optim.param_groups[0]['lr'])\n",
    "                    '''scheduler.step()\n",
    "                    self.losses['LR'].append(scheduler.get_last_lr()[0])'''\n",
    "    \n",
    "                if valid_loss < best_loss:\n",
    "                    self.losses['BState'] = deepcopy(self.model.state_dict())\n",
    "                    best_loss = valid_loss\n",
    "                    print('===========SAVE===========')\n",
    "\n",
    "\n",
    "class Binaryclass(BaseSeqClassifier):#Binaryclass classification\n",
    "    def __init__(self, model, tokenizer, max_len):\n",
    "        super(Binaryclass, self).__init__(model, tokenizer, max_len)\n",
    "\n",
    "    def loss_function(self, input_id, mask, Y, l2_reg):\n",
    "        _, _, pred = self.model(input_id, mask)\n",
    "        bce_loss = nn.BCEWithLogitsLoss()\n",
    "        loss = bce_loss(pred, Y)\n",
    "        return loss\n",
    "\n",
    "    def prdict(self, X, batch_size):\n",
    "        self.model.eval()\n",
    "        encoded_data = self.tokenizer(X, add_special_tokens=True, max_length=self.max_len, padding = 'max_length', truncation=True, return_tensors='pt')\n",
    "        data_load = DataLoader(TensorDataset(encoded_data['input_ids'],  encoded_data['attention_mask']),batch_size=batch_size)\n",
    "        outputs = {'z': [], 'cls_token': [], 'pred': []}\n",
    "        for i, (ids, mask) in enumerate(data_load):\n",
    "            ids, mask = ids.to(device), mask.to(device)\n",
    "            z, cls_token, pred = self.model(ids, mask)\n",
    "            pred = nn.Sigmoid()(pred)\n",
    "            z, cls_token, pred = z.cpu().data.numpy(), cls_token.cpu().data.numpy(), pred.cpu().data.numpy()\n",
    "            outputs['z'].extend(z), outputs['cls_token'].extend(cls_token), outputs['pred'].extend(pred)\n",
    "        return outputs\n",
    "\n",
    "class Multiclass(BaseSeqClassifier):#multiclass classification\n",
    "    def __init__(self, model, tokenizer, max_len):\n",
    "        super(Multiclass, self).__init__(model, tokenizer, max_len)\n",
    "\n",
    "    def loss_function(self, input_id, mask, Y, l2_reg):\n",
    "        _, _, pred = self.model(input_id, mask)\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        loss = ce_loss(pred, Y)\n",
    "        return loss\n",
    "\n",
    "    def prdict(self, X, batch_size):\n",
    "        self.model.eval()\n",
    "        encoded_data = self.tokenizer(X, add_special_tokens=True, max_length=self.max_len, padding = 'max_length', truncation=True, return_tensors='pt')\n",
    "        data_load = DataLoader(TensorDataset(encoded_data['input_ids'],  encoded_data['attention_mask']),batch_size=batch_size)\n",
    "        outputs = {'z': [], 'cls_token': [], 'pred': []}\n",
    "        for i, (ids, mask) in enumerate(data_load):\n",
    "            ids, mask = ids.to(device), mask.to(device)\n",
    "            z, cls_token, pred = self.model(ids, mask)\n",
    "            pred = nn.Softmax()(pred)\n",
    "            z, cls_token, pred = z.cpu().data.numpy(), cls_token.cpu().data.numpy(), pred.cpu().data.numpy()\n",
    "            outputs['z'].extend(z), outputs['cls_token'].extend(cls_token), outputs['pred'].extend(pred)\n",
    "        return outputs\n",
    "\n",
    "def gradient_clipper(model: nn.Module, val: float) -> nn.Module:\n",
    "    def process_grad(grad):\n",
    "        grad[grad != grad] = 1e-10\n",
    "        return torch.clamp(grad, -val, val)\n",
    "    for parameter in model.parameters():\n",
    "        parameter.register_hook(lambda grad: process_grad(grad))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076f294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T01:50:27.368414Z",
     "iopub.status.busy": "2023-04-18T01:50:27.367447Z",
     "iopub.status.idle": "2023-04-18T04:50:42.796489Z",
     "shell.execute_reply": "2023-04-18T04:50:42.794874Z",
     "shell.execute_reply.started": "2023-04-18T01:50:27.368374Z"
    },
    "executionInfo": {
     "elapsed": 13885698,
     "status": "error",
     "timestamp": 1681595800398,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "QSu5FQukSuCY",
    "outputId": "1264785c-bcea-405c-cd81-fd6f2caf74fd",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch, lr, batch_size, d, mlp_d = 50000, 2e-5, 4, 0.000001, 1e-6\n",
    "#cyclic_kwarg = {'base_lr': lr, 'max_lr': 1e-2, 'step_size_up':200, 'step_size_down':200}\n",
    "plateau_kwarg = {'factor':0.5, 'patience':200, 'verbose':True, 'min_lr':1e-7, 'mode':'min'}\n",
    "\n",
    "\n",
    "model = TransSeqClassifier(deberta, model_config.hidden_size, mlp_units=None, mlp_dropout=1e-6, nb_class=df[out_column].nunique())\n",
    "model = gradient_clipper(model, 10)\n",
    "#model.load_state_dict(best_state)\n",
    "print(device)\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "seq_bc = Multiclass(model, tokenizer, 512)\n",
    "seq_bc.fit(X_train.tolist(), Y_train.values, epoch, lr, plateau_kwarg, batch_size=batch_size, grad_clip=10, momentum=0.9,\n",
    "        X_test=X_valid.tolist(), Y_test=Y_valid.values, l2_reg=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b25a0",
   "metadata": {
    "executionInfo": {
     "elapsed": 766,
     "status": "error",
     "timestamp": 1681514840561,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "qcBE63IrqNJO",
    "outputId": "c601d2ed-f88d-4d03-ec0b-489dfb841891",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04175f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T04:50:46.535370Z",
     "iopub.status.busy": "2023-04-18T04:50:46.535000Z",
     "iopub.status.idle": "2023-04-18T04:50:46.669119Z",
     "shell.execute_reply": "2023-04-18T04:50:46.667968Z",
     "shell.execute_reply.started": "2023-04-18T04:50:46.535337Z"
    },
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1681595806258,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "DawdQHTYSv76",
    "outputId": "b5b9ba81-5f43-40a9-c889-dc723d4d37bc",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "best_state = deepcopy(seq_bc.losses['BState'])\n",
    "seq_bc.model.load_state_dict(best_state)\n",
    "print(np.min(seq_bc.losses['Test']))\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1)\n",
    "s = 0\n",
    "fig.append_trace(go.Scatter(x=seq_bc.losses['Epoch'][s:], y=seq_bc.losses['Train'][s:],mode='lines',name='Train'), row=1, col=1)\n",
    "fig.append_trace(go.Scatter(x=seq_bc.losses['Epoch'][s:], y=seq_bc.losses['Test'][s:],mode='lines',name='Test'), row=2, col=1)\n",
    "fig.append_trace(go.Scatter(x=seq_bc.losses['Epoch'][s:], y=seq_bc.losses['LR'][s:],mode='lines',name='LR'), row=3, col=1)\n",
    "fig.update_layout(height=1000, width=1500, title_text=\"Stacked Subplots\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264b5dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T04:50:51.785125Z",
     "iopub.status.busy": "2023-04-18T04:50:51.784106Z",
     "iopub.status.idle": "2023-04-18T04:51:16.582622Z",
     "shell.execute_reply": "2023-04-18T04:51:16.581496Z",
     "shell.execute_reply.started": "2023-04-18T04:50:51.785085Z"
    },
    "executionInfo": {
     "elapsed": 177310,
     "status": "ok",
     "timestamp": 1681595988852,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "jSTTd0YgSx87",
    "outputId": "e3a6af28-0502-423e-dc8b-6543ae512596",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = seq_bc.prdict(X_valid.tolist(), 2)\n",
    "pred = np.argmax(np.asarray(output['pred']), 1)\n",
    "acc = accuracy_score(Y_valid.values, pred)\n",
    "acc, confusion_matrix(Y_valid.values, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2471fd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T04:51:42.200976Z",
     "iopub.status.busy": "2023-04-18T04:51:42.200572Z",
     "iopub.status.idle": "2023-04-18T04:52:06.998615Z",
     "shell.execute_reply": "2023-04-18T04:52:06.997615Z",
     "shell.execute_reply.started": "2023-04-18T04:51:42.200938Z"
    },
    "executionInfo": {
     "elapsed": 184226,
     "status": "ok",
     "timestamp": 1681596199158,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "-WkiZGg7Szga",
    "outputId": "d010efb8-0eb5-413c-b1fd-3f3882560f49",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = seq_bc.prdict(X_test.tolist(), 2)\n",
    "pred = np.argmax(np.asarray(output['pred']), 1)\n",
    "acc = accuracy_score(Y_test.values, pred)\n",
    "acc, confusion_matrix(Y_test.values, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe88ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T04:54:13.608810Z",
     "iopub.status.busy": "2023-04-18T04:54:13.607828Z",
     "iopub.status.idle": "2023-04-18T04:54:13.625208Z",
     "shell.execute_reply": "2023-04-18T04:54:13.623878Z",
     "shell.execute_reply.started": "2023-04-18T04:54:13.608773Z"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1681596213221,
     "user": {
      "displayName": "Youcef Moualek",
      "userId": "10747212200399101970"
     },
     "user_tz": -60
    },
    "id": "7loP5jYqqNJS",
    "outputId": "5c84ea3f-8e25-4b43-eb13-ce608096e1c5",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.concat((X_test.reset_index(drop=True), Y_test.reset_index(drop=True), pd.DataFrame({'Pred': pred.ravel()})), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-18T04:55:27.595851",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}