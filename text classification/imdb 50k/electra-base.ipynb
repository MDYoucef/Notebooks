{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"853d4b3366f1498f818d2d5f6c12a031":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_592590863fcb444a95ac5b5987c43b41","IPY_MODEL_99efd5518f1c4da8ac6f1d7cf8ca79a8","IPY_MODEL_54fdf194ca844acc88282c1e1c1bf26b"],"layout":"IPY_MODEL_8f2018cc86c2446a8f3df291eb98583b"}},"592590863fcb444a95ac5b5987c43b41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83f85147ef2e4f0c8c9c74a4df6c61d0","placeholder":"​","style":"IPY_MODEL_d659006a84cd44dba63b5e27098cf4b9","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"99efd5518f1c4da8ac6f1d7cf8ca79a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c37e2bcb04364a23b468cccb9b859001","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8ade039992b489fa615b20161d35172","value":231508}},"54fdf194ca844acc88282c1e1c1bf26b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d106daf9fcbd4ee0922084d18e0eee20","placeholder":"​","style":"IPY_MODEL_9d5daf45f5d5472394a4309c97d89243","value":" 232k/232k [00:00&lt;00:00, 1.21MB/s]"}},"8f2018cc86c2446a8f3df291eb98583b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83f85147ef2e4f0c8c9c74a4df6c61d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d659006a84cd44dba63b5e27098cf4b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c37e2bcb04364a23b468cccb9b859001":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8ade039992b489fa615b20161d35172":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d106daf9fcbd4ee0922084d18e0eee20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d5daf45f5d5472394a4309c97d89243":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4669ca183a44f3e96c9c2694946e3e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e46a7f58ace4d76955bcf8582eae107","IPY_MODEL_bcc3a33a1d904458ae1d2341995ee419","IPY_MODEL_ccb152659eea46e4bb702448eb759977"],"layout":"IPY_MODEL_e33a3043b67048a580fa009f78024403"}},"4e46a7f58ace4d76955bcf8582eae107":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82fcd8d7345c47a5bee6c705bc9a3ca2","placeholder":"​","style":"IPY_MODEL_b480f3de9bcf4b8388d2978e48d60edc","value":"Downloading (…)okenizer_config.json: 100%"}},"bcc3a33a1d904458ae1d2341995ee419":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_011b234619bc46009d23e4740c5ee450","max":27,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e46083b3b7844279d24aa153188691e","value":27}},"ccb152659eea46e4bb702448eb759977":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb46e53699e74e71a7d206b429e65f01","placeholder":"​","style":"IPY_MODEL_ba64db9bdc1941a5a9d423ff148b5b73","value":" 27.0/27.0 [00:00&lt;00:00, 1.02kB/s]"}},"e33a3043b67048a580fa009f78024403":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82fcd8d7345c47a5bee6c705bc9a3ca2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b480f3de9bcf4b8388d2978e48d60edc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"011b234619bc46009d23e4740c5ee450":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e46083b3b7844279d24aa153188691e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb46e53699e74e71a7d206b429e65f01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba64db9bdc1941a5a9d423ff148b5b73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b5b967a30fd4442b125a7c891c4ea52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d03600e6aefc49bb9bf4228a805195d3","IPY_MODEL_1af6e65365a84b908a9ce69db1b35dc4","IPY_MODEL_3dc435ffffa0416cb1da890299ba1158"],"layout":"IPY_MODEL_cd2b28a28dff4542993b8cf2b4de60ad"}},"d03600e6aefc49bb9bf4228a805195d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6764d385c104ea89f97ad56ec57c881","placeholder":"​","style":"IPY_MODEL_3b0f11b14e3141288c75e0f944343494","value":"Downloading (…)lve/main/config.json: 100%"}},"1af6e65365a84b908a9ce69db1b35dc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ba53c1d1e764dad907caffb5e0b1054","max":666,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b28ad97f7414c58a8faaabe443d7dc4","value":666}},"3dc435ffffa0416cb1da890299ba1158":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1184cd8fef124e81b8f0e5db15acee58","placeholder":"​","style":"IPY_MODEL_41cae9771a844ae19389c06ed4de2422","value":" 666/666 [00:00&lt;00:00, 29.9kB/s]"}},"cd2b28a28dff4542993b8cf2b4de60ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6764d385c104ea89f97ad56ec57c881":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b0f11b14e3141288c75e0f944343494":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ba53c1d1e764dad907caffb5e0b1054":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b28ad97f7414c58a8faaabe443d7dc4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1184cd8fef124e81b8f0e5db15acee58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41cae9771a844ae19389c06ed4de2422":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"adc20405186243a0bc683eeaa0a7abfb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60ac2455e78849be8b0d80c1e41a9243","IPY_MODEL_9349b5c7926b47bcb8378f1f6a9dfca0","IPY_MODEL_b8128867a6414134bbd92a88d79fddc3"],"layout":"IPY_MODEL_bb94068f18c64228b2e36f9c157757a6"}},"60ac2455e78849be8b0d80c1e41a9243":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbfdd95666e042e3894179d271edcdda","placeholder":"​","style":"IPY_MODEL_3d2e5404b81d492485216d2fb2e13a2d","value":"Downloading pytorch_model.bin: 100%"}},"9349b5c7926b47bcb8378f1f6a9dfca0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6388cba4d1e4493b15d8f20454d629f","max":440343552,"min":0,"orientation":"horizontal","style":"IPY_MODEL_459690fb23154499a0d1ef302d0d042c","value":440343552}},"b8128867a6414134bbd92a88d79fddc3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4d4b06df70341f3818ce966ed7b26a6","placeholder":"​","style":"IPY_MODEL_b96337b43d7841cdbbf7f8f7c5063f0e","value":" 440M/440M [00:05&lt;00:00, 84.8MB/s]"}},"bb94068f18c64228b2e36f9c157757a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbfdd95666e042e3894179d271edcdda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d2e5404b81d492485216d2fb2e13a2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6388cba4d1e4493b15d8f20454d629f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"459690fb23154499a0d1ef302d0d042c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4d4b06df70341f3818ce966ed7b26a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b96337b43d7841cdbbf7f8f7c5063f0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["#https://github.com/rainavyas/IMDB_Sentiment_Classification\n","#https://github.com/mohamedsheded/TextClassificationUsingELECTRAtransformer/blob/main/GoEmotions%20(2).ipynb\n","!pip -q install transformers"],"metadata":{"id":"Ow8d3ImlSfI_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4a5123cf-923d-4cb6-905f-2daf8938f73b","execution":{"iopub.status.busy":"2023-04-11T16:15:45.008744Z","iopub.execute_input":"2023-04-11T16:15:45.009621Z","iopub.status.idle":"2023-04-11T16:15:57.242677Z","shell.execute_reply.started":"2023-04-11T16:15:45.009574Z","shell.execute_reply":"2023-04-11T16:15:57.241484Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import torch.optim as opt\n","from transformers import ElectraTokenizer, ElectraModel\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, auc, roc_curve\n","from copy import copy, deepcopy\n","import zipfile\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import os\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # specify which GPU(s) to be used\n","#torch.backends.cudnn.benchmark = True'''"],"metadata":{"id":"ByrHQCFaSbJa","execution":{"iopub.status.busy":"2023-04-11T16:16:10.975439Z","iopub.execute_input":"2023-04-11T16:16:10.975826Z","iopub.status.idle":"2023-04-11T16:16:16.033919Z","shell.execute_reply.started":"2023-04-11T16:16:10.975786Z","shell.execute_reply":"2023-04-11T16:16:16.032790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/kaggle/input/imdb-50k/IMDB Dataset.csv')\n","df"],"metadata":{"id":"-TDb_CRISVOI","colab":{"base_uri":"https://localhost:8080/","height":424},"outputId":"efd183f6-3b22-4dd2-cbf4-0351cbf4e615","execution":{"iopub.status.busy":"2023-04-11T16:16:22.089159Z","iopub.execute_input":"2023-04-11T16:16:22.090162Z","iopub.status.idle":"2023-04-11T16:16:23.432044Z","shell.execute_reply.started":"2023-04-11T16:16:22.090122Z","shell.execute_reply":"2023-04-11T16:16:23.430981Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n0      One of the other reviewers has mentioned that ...  positive\n1      A wonderful little production. <br /><br />The...  positive\n2      I thought this was a wonderful way to spend ti...  positive\n3      Basically there's a family where a little boy ...  negative\n4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n...                                                  ...       ...\n49995  I thought this movie did a down right good job...  positive\n49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n49997  I am a Catholic taught in parochial elementary...  negative\n49998  I'm going to have to disagree with the previou...  negative\n49999  No one expects the Star Trek movies to be high...  negative\n\n[50000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>I thought this movie did a down right good job...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>I am a Catholic taught in parochial elementary...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>I'm going to have to disagree with the previou...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>No one expects the Star Trek movies to be high...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["seq_len = [len(sent) for sent in df['review']]\n","fig = go.Figure()\n","#fig.add_trace(go.Scatter(x=np.arange(len(df)), y=seq_len, mode='markers', name='Seq len'))\n","fig.add_trace(go.Scatter(x=np.arange(len(df)), y=[np.mean(seq_len)]*len(seq_len), mode='lines', name='Avg seq len'))\n","fig.add_trace(go.Scatter(x=np.arange(len(df)), y=[np.median(seq_len)]*len(seq_len), mode='lines', name='Med seq len'))\n","fig.show()"],"metadata":{"id":"cVN2_BwXSnag","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n","\n","'''example_text = ['I will watch Memento tonight']\n","bert_input = tokenizer(example_text,padding='max_length', max_length = 10, truncation=True, return_tensors=\"pt\")\n","tokenizer.decode(bert_input.input_ids[1])'''\n","text_column, out_column = 'review', 'sentiment'\n","labels = dict(zip(df[out_column].unique(), range(df[out_column].nunique())))\n","df.replace({out_column: labels}, inplace=True)\n","df_train, df_valid, df_test = np.split(df.sample(frac=1, random_state=42),  [int(.8*len(df)), int(.9*len(df))])\n","df_train.shape, df_valid.shape, df_test.shape"],"metadata":{"id":"Cj_Yd5UtSpFg","colab":{"base_uri":"https://localhost:8080/","height":130,"referenced_widgets":["853d4b3366f1498f818d2d5f6c12a031","592590863fcb444a95ac5b5987c43b41","99efd5518f1c4da8ac6f1d7cf8ca79a8","54fdf194ca844acc88282c1e1c1bf26b","8f2018cc86c2446a8f3df291eb98583b","83f85147ef2e4f0c8c9c74a4df6c61d0","d659006a84cd44dba63b5e27098cf4b9","c37e2bcb04364a23b468cccb9b859001","e8ade039992b489fa615b20161d35172","d106daf9fcbd4ee0922084d18e0eee20","9d5daf45f5d5472394a4309c97d89243","d4669ca183a44f3e96c9c2694946e3e2","4e46a7f58ace4d76955bcf8582eae107","bcc3a33a1d904458ae1d2341995ee419","ccb152659eea46e4bb702448eb759977","e33a3043b67048a580fa009f78024403","82fcd8d7345c47a5bee6c705bc9a3ca2","b480f3de9bcf4b8388d2978e48d60edc","011b234619bc46009d23e4740c5ee450","6e46083b3b7844279d24aa153188691e","eb46e53699e74e71a7d206b429e65f01","ba64db9bdc1941a5a9d423ff148b5b73","7b5b967a30fd4442b125a7c891c4ea52","d03600e6aefc49bb9bf4228a805195d3","1af6e65365a84b908a9ce69db1b35dc4","3dc435ffffa0416cb1da890299ba1158","cd2b28a28dff4542993b8cf2b4de60ad","e6764d385c104ea89f97ad56ec57c881","3b0f11b14e3141288c75e0f944343494","6ba53c1d1e764dad907caffb5e0b1054","4b28ad97f7414c58a8faaabe443d7dc4","1184cd8fef124e81b8f0e5db15acee58","41cae9771a844ae19389c06ed4de2422","94dec6086e6045eb9e7e3352426c4c01","19b2c3a2642d41519cca3cbfbbbfc213","6794e207b50e4499934228a31330f96a"]},"outputId":"695b4c50-d3dc-4397-bc07-389bbb546d77","execution":{"iopub.status.busy":"2023-04-11T16:16:28.056972Z","iopub.execute_input":"2023-04-11T16:16:28.057700Z","iopub.status.idle":"2023-04-11T16:16:30.323904Z","shell.execute_reply.started":"2023-04-11T16:16:28.057661Z","shell.execute_reply":"2023-04-11T16:16:30.322841Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94dec6086e6045eb9e7e3352426c4c01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19b2c3a2642d41519cca3cbfbbbfc213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6794e207b50e4499934228a31330f96a"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"((40000, 2), (5000, 2), (5000, 2))"},"metadata":{}}]},{"cell_type":"code","source":["len(df[text_column].values[26582]), df[text_column].values[26582]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PB8171TiqNJD","outputId":"9024ff35-4403-4d03-e3d4-fa69c8388d21","trusted":true},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":["(8754,\n"," 'Some have praised -Atlantis:-The Lost Empire- as a Disney adventure for adults. I don\\'t think so--at least not for thinking adults.<br /><br />This script suggests a beginning as a live-action movie, that struck someone as the type of crap you cannot sell to adults anymore. The \"crack staff\" of many older adventure movies has been done well before, (think The Dirty Dozen) but -Atlantis- represents one of the worse films in that motif. The characters are weak. Even the background that each member trots out seems stock and awkward at best. An MD/Medicine Man, a tomboy mechanic whose father always wanted sons, if we have not at least seen these before, we have seen mix-and-match quirks before. The story about how one companion, Vinny played by Don Novello (Fr. Guido Sarducci), went from flower stores to demolitions totally unconvincing.<br /><br />Only the main character, Milo Thatch, a young Atlantis-obsessed academic voiced by Michael J. Fox, has any depth to him. Milo\\'s search for Atlantis continues that of his grandfather who raised him. The opening scene shows a much younger Milo giddily perched on a knee, as his grandfather places his pith helmet on his head.<br /><br />And while the characters were thin at best, the best part about -Atlantis- was the voice talent. Commander Rourke loses nothing being voiced by James Garner. Although Rourke is a pretty stock military type, Garner shows his ability to breath life into characters simply by his delivery. Garner\\'s vocal performance is the high point. I\\'m sorry to say Leonard Nimoy\\'s Dying King is nothing more than obligatory. Additionally, Don Novello as the demolition expert, Vinny Santorini, was also notable for one or two well-done, funny lines--but I\\'ve always liked Father Guido Sarducci, anyway.<br /><br />Also well done was the Computer Animation. The BACKGROUND animation, that is. The character animation does nothing if not make already flat characters appear even flatter. Aside from landscapes, buildings and vehicles there isn\\'t much to impress.<br /><br />The plot was the worst. Some say hackneyed or trite. I\\'m not so sure about that. Any serviceable plot can be made into something new with the proper treatment. Shakespeare often started from a known story and plot and was famous only for putting on a new coat of paint. So the treatment is the thing. And -Atlantis- obviously lacks that.<br /><br />I cannot begin to go into all the logic gaps without a spoiler section. The plot was bad. The plot\\'s bridges snap like twine and the ending does not make sense. To add to that, the script and the animation is peppered with annoying sloppiness.<br /><br />** SPOILERS **<br /><br />Right at the beginning when Milo reveals that runic or Celtic symbols have been wrongly transliterated and the \"Coast of Ireland\" should read the \"Coast of Iceland\", we begin to have problems. The writers of the script would need to know the British take for Eire or Eireann as \"Ireland\", and completely ignore the older, Latin term Hibernia. But more than this, they need to know of the Vikings conspiracy to call the greener island Iceland and the icier island Greenland.<br /><br />By making it the matter of a mis-tranliterated \"letter\", the writers have doomed themselves to requiring a runic version of English and a post-Roman date on the script. Since this is long after Atlantis was supposed to have sunk into its undersea cave. And without visible clues and less technology than Milo had, made the inscription far less trustworthy.<br /><br />The Shepherd\\'s Journal could not be written before the sinking of Atlantis, or it would know nothing about the cave or the crystal lying \"in the King\\'s eye\". It must have been written after the sinking, but without even the technology that Milo\\'s expedition had, how the heck did anybody get by the Leviathan. So how could it know more about anything after that? And why would it be written in Atlantian?<br /><br />Automatic writing and clairvoyance or astral travel can explain these things. However clairvoyance and astral travel do not require to write in Atlantian. So it\\'s got to be some sort automatic writing. Since no-one left in Atlantis can read, it must be the spirits of the crystal beaming messages to the surface. This would have made more sense. But could also have been explained within the movie: Milo could shepherd have discovered that this power had been calling him all his life--appeared in dreams, etc. This needed to be explored in the movie.<br /><br />The Atlantians should simply not be able to comprehend modern languages. No-one expects that the original Indo-Europeans would be able to converse in Europe, anymore than Romans would understand that hard \"c\"s or their day became French \"ch\"s (pronounced like \"sh\"s, no less!)<br /><br />Current Atlantians were alive before the cataclysm--when apparently they *could* read, yet now are unable to read what they used to, or operate similar machinery.<br /><br />The Mass Illiteracy points out a crucial flaw in the movie. NOTHING seems to have happened to this culture. It seems suspended in air until Milo can rescue it. Even though it appears that life is not a constant struggle for survival, no-one wants to compose poetry or write novels and perhaps it is a combination of Atlantian school systems going downhill toward the end and lack of good fiction that caused Atlantis to fall into illiteracy.<br /><br />Kida can be excused for not knowing how to read or operate the machinery if she was so young when the Cataclysm of Stupidity set in--But ANY OF IT **HARDLY** qualifies her father for Deification!! Kashakim\\'s foolishness almost single-handedly wiped his people from existence. Killed a bunch in the cataclysm, stalled progress (not a lot killed here, but he oversaw a massive slide in culture and progress) until someone could take the crystal to kill everybody, if they weren\\'t boiled in lava first because the Giant Robots weren\\'t there to protect them.<br /><br />A bolt of blue electricity should have shattered Kashakim\\'s likeness, when Kida tried joining her father\\'s image to the circle of GREAT Kings of Atlantis!<br /><br />Even though Milo was the only one who could read Atlantian, Rourke and others knew enough to look through a book of gibberish and find a page on a crystal--which he knew to be a crystal and not some stylized astrological or \"phases of the sun\" diagram.<br /><br />If Milo\\'s grandfather had told Rourke about it, it still does not explain how Rourke would have suffered from Milo\\'s reading it as part of the book. Ripping out the page--which was dog-eared in Rourke\\'s hand, even though Milo found NO sign of a torn page in the book apparently--only was there to tip off the viewer that \"something was not quite right\". Unless the word \"crystal\" would have set alarms off in Milo\\'s head that somebody would try to steal it, Milo would have suspected nothing. It\\'s just thick-headed foreshadowing.<br /><br />The crew\\'s \"double-cross\" was not a character change. We learned that Vinny, Sweet, Audrey and Cookie had been going along with Rourke from the beginning. However, the \"change of heart\" falls flat. It was a change, and needed to be better motivated. Hard to do with characters who weren\\'t given anything to begin with.<br /><br />Niggling little bit that the lava flows up over the dome, instead of filling in the rest of the area that we view the sequence from. It\\'s liquid; it will not flow over the protective dome until it fills up all lower areas.<br /><br />The ending STINKS!-- and makes no sense other than to appease political correctness. With it\\'s powersource restored, Atlantis is no longer a weak power, needing coddling. The giant robot guardians and the sky-cycles shooting blue lightning suggest that they have less to fear from us than they might. The technology is superior to ours, and definitely to early 20th-century. In the end Milo needs to teach the Atlantians to read, for what? The whole idea is to leave their little quiet, chastened culture alone, not to send it into hyperdrive.<br /><br />** END SPOILERS **<br /><br />Perhaps, the Lost World plot and the turn-of-the-century setting should give me a hint that this is more an homage to pulps. The failures I find with the film agree with this idea. But I am at a loss why I should pay to see thin characters and plot holes simply because many dime novels had them as well. And pulp stories is part of the \"crap they can\\'t sell adults anymore\", anyway. We have become a bit more sophisticated and our pulp needs to grow up as well. Raiders of the Lost Ark lost none of its pulp feel and avoided so much badness.<br /><br />4 out of 10--the movie is enjoyable but as I think about the plot, it seeps ever lower.')"]},"metadata":{}}]},{"cell_type":"code","source":["tmp = tokenizer([df[text_column].values[26582]], padding = 'max_length', truncation=True, return_tensors='pt')\n","tmp['input_ids'].shape, tmp['input_ids'], tmp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KbykiD5mqNJE","outputId":"14fcf4e9-7c97-42d3-8743-784bb426d25c","trusted":true},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":["(torch.Size([1, 512]),\n"," tensor([[  101,  2070,  2031,  5868,  1011, 16637,  1024,  1011,  1996,  2439,\n","           3400,  1011,  2004,  1037,  6373,  6172,  2005,  6001,  1012,  1045,\n","           2123,  1005,  1056,  2228,  2061,  1011,  1011,  2012,  2560,  2025,\n","           2005,  3241,  6001,  1012,  1026,  7987,  1013,  1028,  1026,  7987,\n","           1013,  1028,  2023,  5896,  6083,  1037,  2927,  2004,  1037,  2444,\n","           1011,  2895,  3185,  1010,  2008,  4930,  2619,  2004,  1996,  2828,\n","           1997, 10231,  2017,  3685,  5271,  2000,  6001,  4902,  1012,  1996,\n","           1000,  8579,  3095,  1000,  1997,  2116,  3080,  6172,  5691,  2038,\n","           2042,  2589,  2092,  2077,  1010,  1006,  2228,  1996,  6530,  6474,\n","           1007,  2021,  1011, 16637,  1011,  5836,  2028,  1997,  1996,  4788,\n","           3152,  1999,  2008, 16226,  1012,  1996,  3494,  2024,  5410,  1012,\n","           2130,  1996,  4281,  2008,  2169,  2266, 19817, 12868,  2041,  3849,\n","           4518,  1998,  9596,  2012,  2190,  1012,  2019,  9108,  1013,  4200,\n","           2158,  1010,  1037,  8136,  6977, 15893,  3005,  2269,  2467,  2359,\n","           4124,  1010,  2065,  2057,  2031,  2025,  2012,  2560,  2464,  2122,\n","           2077,  1010,  2057,  2031,  2464,  4666,  1011,  1998,  1011,  2674,\n","          21864, 19987,  2077,  1012,  1996,  2466,  2055,  2129,  2028,  7452,\n","           1010, 19354,  4890,  2209,  2011,  2123,  3117,  4135,  1006, 10424,\n","           1012, 20239, 18906,  8566, 14693,  1007,  1010,  2253,  2013,  6546,\n","           5324,  2000, 12451,  2015,  6135,  4895,  8663,  6371,  6129,  1012,\n","           1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2069,  1996,\n","           2364,  2839,  1010, 20359,  2008,  2818,  1010,  1037,  2402, 16637,\n","           1011, 15896,  3834,  6126,  2011,  2745,  1046,  1012,  4419,  1010,\n","           2038,  2151,  5995,  2000,  2032,  1012, 20359,  1005,  1055,  3945,\n","           2005, 16637,  4247,  2008,  1997,  2010,  5615,  2040,  2992,  2032,\n","           1012,  1996,  3098,  3496,  3065,  1037,  2172,  3920, 20359, 21025,\n","          14141,  6588, 17335,  2006,  1037,  6181,  1010,  2004,  2010,  5615,\n","           3182,  2010,  6770,  2232, 10412,  2006,  2010,  2132,  1012,  1026,\n","           7987,  1013,  1028,  1026,  7987,  1013,  1028,  1998,  2096,  1996,\n","           3494,  2020,  4857,  2012,  2190,  1010,  1996,  2190,  2112,  2055,\n","           1011, 16637,  1011,  2001,  1996,  2376,  5848,  1012,  3474, 24400,\n","          12386,  2498,  2108,  6126,  2011,  2508, 18661,  1012,  2348, 24400,\n","           2003,  1037,  3492,  4518,  2510,  2828,  1010, 18661,  3065,  2010,\n","           3754,  2000,  3052,  2166,  2046,  3494,  3432,  2011,  2010,  6959,\n","           1012, 18661,  1005,  1055,  5554,  2836,  2003,  1996,  2152,  2391,\n","           1012,  1045,  1005,  1049,  3374,  2000,  2360,  7723,  9152,  5302,\n","           2100,  1005,  1055,  5996,  2332,  2003,  2498,  2062,  2084, 26471,\n","           1012,  5678,  1010,  2123,  3117,  4135,  2004,  1996, 12451,  6739,\n","           1010, 19354,  4890, 11685, 22612,  1010,  2001,  2036,  3862,  2005,\n","           2028,  2030,  2048,  2092,  1011,  2589,  1010,  6057,  3210,  1011,\n","           1011,  2021,  1045,  1005,  2310,  2467,  4669,  2269, 20239, 18906,\n","           8566, 14693,  1010,  4312,  1012,  1026,  7987,  1013,  1028,  1026,\n","           7987,  1013,  1028,  2036,  2092,  2589,  2001,  1996,  3274,  7284,\n","           1012,  1996,  4281,  7284,  1010,  2008,  2003,  1012,  1996,  2839,\n","           7284,  2515,  2498,  2065,  2025,  2191,  2525,  4257,  3494,  3711,\n","           2130,  4257,  3334,  1012,  4998,  2013, 12793,  1010,  3121,  1998,\n","           4683,  2045,  3475,  1005,  1056,  2172,  2000, 17894,  1012,  1026,\n","           7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,  5436,  2001,\n","           1996,  5409,  1012,  2070,  2360, 28425,  2098,  2030, 13012,  2618,\n","           1012,  1045,  1005,  1049,  2025,  2061,  2469,  2055,  2008,  1012,\n","           2151,  2326,  3085,  5436,  2064,  2022,  2081,  2046,  2242,  2047,\n","           2007,   102]]),\n"," {'input_ids': tensor([[  101,  2070,  2031,  5868,  1011, 16637,  1024,  1011,  1996,  2439,\n","           3400,  1011,  2004,  1037,  6373,  6172,  2005,  6001,  1012,  1045,\n","           2123,  1005,  1056,  2228,  2061,  1011,  1011,  2012,  2560,  2025,\n","           2005,  3241,  6001,  1012,  1026,  7987,  1013,  1028,  1026,  7987,\n","           1013,  1028,  2023,  5896,  6083,  1037,  2927,  2004,  1037,  2444,\n","           1011,  2895,  3185,  1010,  2008,  4930,  2619,  2004,  1996,  2828,\n","           1997, 10231,  2017,  3685,  5271,  2000,  6001,  4902,  1012,  1996,\n","           1000,  8579,  3095,  1000,  1997,  2116,  3080,  6172,  5691,  2038,\n","           2042,  2589,  2092,  2077,  1010,  1006,  2228,  1996,  6530,  6474,\n","           1007,  2021,  1011, 16637,  1011,  5836,  2028,  1997,  1996,  4788,\n","           3152,  1999,  2008, 16226,  1012,  1996,  3494,  2024,  5410,  1012,\n","           2130,  1996,  4281,  2008,  2169,  2266, 19817, 12868,  2041,  3849,\n","           4518,  1998,  9596,  2012,  2190,  1012,  2019,  9108,  1013,  4200,\n","           2158,  1010,  1037,  8136,  6977, 15893,  3005,  2269,  2467,  2359,\n","           4124,  1010,  2065,  2057,  2031,  2025,  2012,  2560,  2464,  2122,\n","           2077,  1010,  2057,  2031,  2464,  4666,  1011,  1998,  1011,  2674,\n","          21864, 19987,  2077,  1012,  1996,  2466,  2055,  2129,  2028,  7452,\n","           1010, 19354,  4890,  2209,  2011,  2123,  3117,  4135,  1006, 10424,\n","           1012, 20239, 18906,  8566, 14693,  1007,  1010,  2253,  2013,  6546,\n","           5324,  2000, 12451,  2015,  6135,  4895,  8663,  6371,  6129,  1012,\n","           1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2069,  1996,\n","           2364,  2839,  1010, 20359,  2008,  2818,  1010,  1037,  2402, 16637,\n","           1011, 15896,  3834,  6126,  2011,  2745,  1046,  1012,  4419,  1010,\n","           2038,  2151,  5995,  2000,  2032,  1012, 20359,  1005,  1055,  3945,\n","           2005, 16637,  4247,  2008,  1997,  2010,  5615,  2040,  2992,  2032,\n","           1012,  1996,  3098,  3496,  3065,  1037,  2172,  3920, 20359, 21025,\n","          14141,  6588, 17335,  2006,  1037,  6181,  1010,  2004,  2010,  5615,\n","           3182,  2010,  6770,  2232, 10412,  2006,  2010,  2132,  1012,  1026,\n","           7987,  1013,  1028,  1026,  7987,  1013,  1028,  1998,  2096,  1996,\n","           3494,  2020,  4857,  2012,  2190,  1010,  1996,  2190,  2112,  2055,\n","           1011, 16637,  1011,  2001,  1996,  2376,  5848,  1012,  3474, 24400,\n","          12386,  2498,  2108,  6126,  2011,  2508, 18661,  1012,  2348, 24400,\n","           2003,  1037,  3492,  4518,  2510,  2828,  1010, 18661,  3065,  2010,\n","           3754,  2000,  3052,  2166,  2046,  3494,  3432,  2011,  2010,  6959,\n","           1012, 18661,  1005,  1055,  5554,  2836,  2003,  1996,  2152,  2391,\n","           1012,  1045,  1005,  1049,  3374,  2000,  2360,  7723,  9152,  5302,\n","           2100,  1005,  1055,  5996,  2332,  2003,  2498,  2062,  2084, 26471,\n","           1012,  5678,  1010,  2123,  3117,  4135,  2004,  1996, 12451,  6739,\n","           1010, 19354,  4890, 11685, 22612,  1010,  2001,  2036,  3862,  2005,\n","           2028,  2030,  2048,  2092,  1011,  2589,  1010,  6057,  3210,  1011,\n","           1011,  2021,  1045,  1005,  2310,  2467,  4669,  2269, 20239, 18906,\n","           8566, 14693,  1010,  4312,  1012,  1026,  7987,  1013,  1028,  1026,\n","           7987,  1013,  1028,  2036,  2092,  2589,  2001,  1996,  3274,  7284,\n","           1012,  1996,  4281,  7284,  1010,  2008,  2003,  1012,  1996,  2839,\n","           7284,  2515,  2498,  2065,  2025,  2191,  2525,  4257,  3494,  3711,\n","           2130,  4257,  3334,  1012,  4998,  2013, 12793,  1010,  3121,  1998,\n","           4683,  2045,  3475,  1005,  1056,  2172,  2000, 17894,  1012,  1026,\n","           7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,  5436,  2001,\n","           1996,  5409,  1012,  2070,  2360, 28425,  2098,  2030, 13012,  2618,\n","           1012,  1045,  1005,  1049,  2025,  2061,  2469,  2055,  2008,  1012,\n","           2151,  2326,  3085,  5436,  2064,  2022,  2081,  2046,  2242,  2047,\n","           2007,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1]])})"]},"metadata":{}}]},{"cell_type":"code","source":["tokenizer.decode(tmp['input_ids'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"A84yRfMDqNJG","outputId":"6c2b91b0-7cf2-45b8-aa66-0288e563e33e","trusted":true},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":["'[CLS] some have praised - atlantis : - the lost empire - as a disney adventure for adults. i don\\'t think so - - at least not for thinking adults. < br / > < br / > this script suggests a beginning as a live - action movie, that struck someone as the type of crap you cannot sell to adults anymore. the \" crack staff \" of many older adventure movies has been done well before, ( think the dirty dozen ) but - atlantis - represents one of the worse films in that motif. the characters are weak. even the background that each member trots out seems stock and awkward at best. an md / medicine man, a tomboy mechanic whose father always wanted sons, if we have not at least seen these before, we have seen mix - and - match quirks before. the story about how one companion, vinny played by don novello ( fr. guido sarducci ), went from flower stores to demolitions totally unconvincing. < br / > < br / > only the main character, milo thatch, a young atlantis - obsessed academic voiced by michael j. fox, has any depth to him. milo\\'s search for atlantis continues that of his grandfather who raised him. the opening scene shows a much younger milo giddily perched on a knee, as his grandfather places his pith helmet on his head. < br / > < br / > and while the characters were thin at best, the best part about - atlantis - was the voice talent. commander rourke loses nothing being voiced by james garner. although rourke is a pretty stock military type, garner shows his ability to breath life into characters simply by his delivery. garner\\'s vocal performance is the high point. i\\'m sorry to say leonard nimoy\\'s dying king is nothing more than obligatory. additionally, don novello as the demolition expert, vinny santorini, was also notable for one or two well - done, funny lines - - but i\\'ve always liked father guido sarducci, anyway. < br / > < br / > also well done was the computer animation. the background animation, that is. the character animation does nothing if not make already flat characters appear even flatter. aside from landscapes, buildings and vehicles there isn\\'t much to impress. < br / > < br / > the plot was the worst. some say hackneyed or trite. i\\'m not so sure about that. any serviceable plot can be made into something new with [SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}}]},{"cell_type":"code","source":["df_train"],"metadata":{"id":"k27YMJY56ksb","outputId":"f76875b6-ef3f-487f-d942-393340fa6c74","colab":{"base_uri":"https://localhost:8080/","height":424},"execution":{"iopub.status.busy":"2023-04-11T16:16:39.423293Z","iopub.execute_input":"2023-04-11T16:16:39.424327Z","iopub.status.idle":"2023-04-11T16:16:39.438751Z","shell.execute_reply.started":"2023-04-11T16:16:39.424279Z","shell.execute_reply":"2023-04-11T16:16:39.437640Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                  review  sentiment\n33553  I really liked this Summerslam due to the look...          0\n9427   Not many television shows appeal to quite as m...          0\n199    The film quickly gets to a major chase scene w...          1\n12447  Jane Austen would definitely approve of this o...          0\n39489  Expectations were somewhat high for me when I ...          1\n...                                                  ...        ...\n1559   This kind of \"inspirational\" saccharine is eno...          1\n13313  When people nowadays hear of a 1940s drama, th...          0\n13528  This is a low budget Roger Corman horror/creat...          1\n25017  First off, let it be known that I came into th...          0\n19317  Basil Rathbone and Nigel Bruce as Sherlock Hol...          0\n\n[40000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>33553</th>\n      <td>I really liked this Summerslam due to the look...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9427</th>\n      <td>Not many television shows appeal to quite as m...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>The film quickly gets to a major chase scene w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12447</th>\n      <td>Jane Austen would definitely approve of this o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39489</th>\n      <td>Expectations were somewhat high for me when I ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1559</th>\n      <td>This kind of \"inspirational\" saccharine is eno...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13313</th>\n      <td>When people nowadays hear of a 1940s drama, th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13528</th>\n      <td>This is a low budget Roger Corman horror/creat...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25017</th>\n      <td>First off, let it be known that I came into th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19317</th>\n      <td>Basil Rathbone and Nigel Bruce as Sherlock Hol...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class SeqClsLoader(Dataset): #Sequence classification dataloader\n","    def __init__(self, encoded_data, labels):\n","        self.labels = labels\n","        self.encoded_data = encoded_data\n","\n","    def classes(self):\n","        return self.labels\n"," \n","    def __len__(self):\n","        return len(self.encoded_data['input_ids'])\n"," \n","    def __getitem__(self, idx):\n","        return self.encoded_data['input_ids'][idx], self.encoded_data['attention_mask'][idx], self.labels[idx] if self.labels is not None else None\n","\n","class NNModel(nn.Module):\n","    def __init__(self, input_shape, units=None, factors=None, activ=True, norm=False, dropout=False, slops=None):\n","        super().__init__()\n","        self.input_shape = input_shape\n","        self.units = units\n","        self.factors = factors\n","        self.activ, self.norm = activ, norm\n","        self.network = nn.ModuleList()\n","        if self.factors:\n","            self.units = np.round(self.input_shape * np.asarray(self.factors)).astype(int)\n","        if self.units is not None:\n","            self.dropout = np.zeros_like(self.units) if not dropout else dropout\n","            self.slops = np.full(len(self.units), 1) if slops is None else slops\n","            for i, j, k in zip(self.units, self.dropout, self.slops):\n","                if i >= 1:\n","                    block = self.__build_block__(input_shape, i, p=j, slop=k)\n","                    self.network.extend(block)\n","                    input_shape = i\n","        self.output_shape = input_shape\n","        self.reset_parameters()\n","    \n","    def __build_block__(self, input_shape, units, p, slop):\n","        block = []\n","        block.append(nn.Linear(input_shape, units, bias=not self.norm))\n","        if self.norm:\n","            block.append(nn.BatchNorm1d(units))\n","            #block.append(nn.LayerNorm(units, eps=1e-5))\n","        if self.activ:\n","            #block.append(nn.LeakyReLU())\n","            block.append(nn.ELU(slop))\n","            #block.append(nn.GELU())\n","        if p > 0:\n","            block.append(nn.Dropout(p))\n","        return block\n"," \n","    def forward(self, x):\n","        for layer in self.network:\n","          tmp = layer(x)\n","          x = tmp\n","        return x\n"," \n","    def reset_parameters(self):\n","        for layer in self.network:\n","            if isinstance(layer, nn.Linear):\n","                nn.init.xavier_normal_(layer.weight)\n","                layer.bias.data.fill_(0.1)\n"," \n","\n","class TransSeqClassifier(nn.Module):\n","    def __init__(self, model, emb_dim, mlp_units, mlp_dropout, nb_class):\n","        super(TransSeqClassifier, self).__init__()\n","        self.model = model\n","        self.mlp = NNModel(emb_dim, units=mlp_units, factors=None, dropout=[mlp_dropout]*len(mlp_units)) if mlp_units is not None else None\n","        cls_units = self.mlp.output_shape if mlp_units is not None else emb_dim\n","        self.classifier = nn.Linear(cls_units, nb_class)\n","\n","    def forward(self, input_id, mask):\n","        discriminator_hidden_states = self.model(input_ids= input_id, attention_mask=mask,return_dict=False)\n","        last_hidden_state = discriminator_hidden_states[0]\n","        cls_token = last_hidden_state[:, 0, :]  # take <s> token (equiv. to [CLS])\n","        z = self.mlp(cls_token) if self.mlp is not None else cls_token\n","        pred = self.classifier(z)\n","        return z, cls_token, pred\n","\n","class BaseSeqClassifier:\n","    def __init__(self, model, tokenizer):\n","        self.model = model.to(device)\n","        self.losses = {'Epoch': [], 'Train': [], 'Test': [], 'BState': [], 'LState': [], 'LR': []}\n","        self.tokenizer = tokenizer\n"," \n","    def train_model(self, optim, train_loader, grad_clip, l2_reg):\n","          total_loss = 0\n","          self.model = self.model.train()\n","        #with autograd.detect_anomaly():\n","          for i, (ids, mask, Y) in enumerate(train_loader):\n","              ids, mask, Y = ids.to(device), mask.to(device), Y.to(device)\n","              #self.model.get_weight()\n","              optim.zero_grad()\n","              loss = self.loss_function(ids, mask, Y, l2_reg)\n","              loss.backward()\n","              torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n","              optim.step()\n","              total_loss += loss.item()\n","          return total_loss/(i+1)\n","        \n"," \n","    def eval_model(self, test_loader):\n","        self.model = self.model.eval()\n","        total_loss = 0\n","        for i, (ids, mask, Y) in enumerate(test_loader):\n","            ids, mask, Y = ids.to(device), mask.to(device), Y.to(device)\n","            loss = self.loss_function(ids, mask, Y, l2_reg=0)\n","            total_loss += loss.item()\n","        return total_loss/(i+1)#np.abs(-100. - total_loss)\n"," \n","    def fit(self, X_train, Y_train, epoch, lr, opt_kwarg, batch_size=None,  grad_clip=100, momentum=0.9, X_test=None, Y_test=None, l2_reg=0, verbose=True, save=True):\n","        batch_size = len(Y_train) if batch_size is None else batch_size\n","        encoded_train = self.tokenizer(X_train, padding = 'max_length', truncation=True, return_tensors='pt')\n","        train_load = DataLoader(SeqClsLoader(encoded_train, Y_train), batch_size=batch_size, shuffle=True)  # DATALOADER obj\n","        if X_test is not None:\n","            encoded_test = self.tokenizer(X_test, padding = 'max_length', truncation=True, return_tensors='pt')\n","            test_load = DataLoader(SeqClsLoader(encoded_test, Y_test), batch_size=batch_size, shuffle=True)\n"," \n","        best_loss = 1e100\n","        optim = opt.Adam(self.model.parameters(), lr=lr)\n","        #optim = opt.SGD(self.model.parameters(), lr=lr, momentum=momentum, nesterov=True)\n","\n","        scheduler = None\n","        #scheduler = opt.lr_scheduler.CyclicLR(optim, **opt_kwarg)\n","        #scheduler = opt.lr_scheduler.ReduceLROnPlateau(optim, **opt_kwarg)\n","        #scheduler = opt.lr_scheduler.MultiStepLR(optim, milestones=[28, 120], gamma=0.1)\n","\n","        eval_score = ''\n","        for i in range(epoch):\n","            if verbose:\n","                print('##### EPOCH ' + str(i) + ' #####')\n","               \n","            train_loss = self.train_model(optim, train_load, grad_clip, l2_reg)\n","            self.losses['LState'] = deepcopy(self.model.state_dict())\n","    \n","            if verbose:\n","                print('train loss : ', train_loss)\n","            self.losses['Epoch'].append(i), self.losses['Train'].append(train_loss)\n","    \n","            if df_test is not None:\n","                valid_loss = self.eval_model(test_load)\n","\n","                if verbose:\n","                    print('test loss : ', valid_loss)\n","                self.losses['Test'].append(valid_loss)\n","    \n","                if scheduler is not None:\n","                    scheduler.step(valid_loss)\n","                    self.losses['LR'].append(optim.param_groups[0]['lr'])\n","                    '''scheduler.step()\n","                    self.losses['LR'].append(scheduler.get_last_lr()[0])'''\n","    \n","                if valid_loss < best_loss:\n","                    self.losses['BState'] = deepcopy(self.model.state_dict())\n","                    best_loss = valid_loss\n","                    print('===========SAVE===========')\n","\n","\n","class Binaryclass(BaseSeqClassifier):#Binaryclass classification\n","    def __init__(self, model, tokenizer):\n","        super(Binaryclass, self).__init__(model, tokenizer,)\n","\n","    def loss_function(self, input_id, mask, Y, l2_reg):\n","        _, _, pred = self.model(input_id, mask)\n","        bce_loss = nn.BCEWithLogitsLoss()\n","        loss = bce_loss(pred, Y)\n","        return loss\n","\n","    def prdict(self, X, batch_size):\n","        self.model.eval()\n","        encoded_data = self.tokenizer(X, add_special_tokens=True, return_attention_mask=True, pad_to_max_length=True, max_length=512, return_tensors='pt')\n","        data_load = DataLoader(TensorDataset(encoded_data['input_ids'],  encoded_data['attention_mask']),batch_size=batch_size)\n","        outputs = {'z': [], 'pooled_output': [], 'pred': []}\n","        for i, (ids, mask) in enumerate(data_load):\n","            ids, mask = ids.to(device), mask.to(device)\n","            z, pooled_output, pred = self.model(ids, mask)\n","            pred = nn.Sigmoid()(pred)\n","            z, pooled_output, pred = z.cpu().data.numpy(), pooled_output.cpu().data.numpy(), pred.cpu().data.numpy()\n","            outputs['z'].extend(z), outputs['pooled_output'].extend(pooled_output), outputs['pred'].extend(pred)\n","        return outputs\n","\n","class Multiclass(BaseSeqClassifier):#multiclass classification\n","    def __init__(self, model, tokenizer):\n","        super(Multiclass, self).__init__(model, tokenizer,)\n","\n","    def loss_function(self, input_id, mask, Y, l2_reg):\n","        _, _, pred = self.model(input_id, mask)\n","        ce_loss = nn.CrossEntropyLoss()\n","        loss = ce_loss(pred, Y)\n","        return loss\n","\n","    def prdict(self, X, batch_size):\n","        self.model.eval()\n","        encoded_data = self.tokenizer(X, add_special_tokens=True, return_attention_mask=True, pad_to_max_length=True, max_length=512, return_tensors='pt')\n","        data_load = DataLoader(TensorDataset(encoded_data['input_ids'],  encoded_data['attention_mask']),batch_size=batch_size)\n","        outputs = {'z': [], 'pooled_output': [], 'pred': []}\n","        for i, (ids, mask) in enumerate(data_load):\n","            ids, mask = ids.to(device), mask.to(device)\n","            z, pooled_output, pred = self.model(ids, mask)\n","            pred = nn.Softmax()(pred)\n","            z, pooled_output, pred = z.cpu().data.numpy(), pooled_output.cpu().data.numpy(), pred.cpu().data.numpy()\n","            outputs['z'].extend(z), outputs['pooled_output'].extend(pooled_output), outputs['pred'].extend(pred)\n","        return outputs\n","\n","def gradient_clipper(model: nn.Module, val: float) -> nn.Module:\n","    def process_grad(grad):\n","        grad[grad != grad] = 1e-10\n","        return torch.clamp(grad, -val, val)\n","    for parameter in model.parameters():\n","        parameter.register_hook(lambda grad: process_grad(grad))\n","    \n","    return model"],"metadata":{"id":"-9SHWS_lSr0R","execution":{"iopub.status.busy":"2023-04-11T16:16:42.638070Z","iopub.execute_input":"2023-04-11T16:16:42.638489Z","iopub.status.idle":"2023-04-11T16:16:42.743015Z","shell.execute_reply.started":"2023-04-11T16:16:42.638452Z","shell.execute_reply":"2023-04-11T16:16:42.742025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch, lr, batch_size, d, mlp_d = 50000, 1e-6, 8, 0.000001, 1e-6\n","#cyclic_kwarg = {'base_lr': lr, 'max_lr': 1e-2, 'step_size_up':200, 'step_size_down':200}\n","plateau_kwarg = {'factor':0.5, 'patience':200, 'verbose':True, 'min_lr':1e-7, 'mode':'min'}\n","\n","electra = ElectraModel.from_pretrained(\"google/electra-base-discriminator\")\n","model = TransSeqClassifier(electra, 768, mlp_units=None, mlp_dropout=1e-6, nb_class=1)\n","model = gradient_clipper(model, 10)\n","#nn_model.load_state_dict(best_state)\n","print(device)\n","print(model)\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n","seq_bc = Binaryclass(model, tokenizer)\n","seq_bc.fit(df_train[text_column].tolist(), df_train[out_column].values.astype(np.float32)[:,None], epoch, lr, plateau_kwarg, batch_size=batch_size, grad_clip=10, momentum=0.9,\n","        X_test=df_valid[text_column].tolist(), Y_test=df_valid[out_column].values.astype(np.float32)[:,None], l2_reg=0, verbose=True)"],"metadata":{"id":"QSu5FQukSuCY","colab":{"base_uri":"https://localhost:8080/","height":868,"referenced_widgets":["adc20405186243a0bc683eeaa0a7abfb","60ac2455e78849be8b0d80c1e41a9243","9349b5c7926b47bcb8378f1f6a9dfca0","b8128867a6414134bbd92a88d79fddc3","bb94068f18c64228b2e36f9c157757a6","fbfdd95666e042e3894179d271edcdda","3d2e5404b81d492485216d2fb2e13a2d","d6388cba4d1e4493b15d8f20454d629f","459690fb23154499a0d1ef302d0d042c","a4d4b06df70341f3818ce966ed7b26a6","b96337b43d7841cdbbf7f8f7c5063f0e","ebfed40e672a4d87b10e7cbd8d0ec7c3"]},"outputId":"08cde3cb-4c7a-4ea6-fe7c-5532594611c1","execution":{"iopub.status.busy":"2023-04-11T16:16:48.813808Z","iopub.execute_input":"2023-04-11T16:16:48.814838Z","iopub.status.idle":"2023-04-11T19:04:27.051510Z","shell.execute_reply.started":"2023-04-11T16:16:48.814787Z","shell.execute_reply":"2023-04-11T19:04:27.048582Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebfed40e672a4d87b10e7cbd8d0ec7c3"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"cuda\nTransSeqClassifier(\n  (model): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n)\n108892417\n##### EPOCH 0 #####\ntrain loss :  0.2301355963565409\ntest loss :  0.15782910123765467\n===========SAVE===========\n##### EPOCH 1 #####\ntrain loss :  0.1402164114361629\ntest loss :  0.1459776452176273\n===========SAVE===========\n##### EPOCH 2 #####\ntrain loss :  0.11969724256759509\ntest loss :  0.14809688719436526\n##### EPOCH 3 #####\ntrain loss :  0.1035334189969115\ntest loss :  0.1495963387325406\n##### EPOCH 4 #####\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1285725422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mseq_bc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m seq_bc.fit(df_train[text_column].tolist(), df_train[out_column].values.astype(np.float32)[:,None], epoch, lr, plateau_kwarg, batch_size=batch_size, grad_clip=10, momentum=0.9,\n\u001b[0;32m---> 14\u001b[0;31m         X_test=df_valid[text_column].tolist(), Y_test=df_valid[out_column].values.astype(np.float32)[:,None], l2_reg=0, verbose=True)\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/1948464976.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, Y_train, epoch, lr, opt_kwarg, batch_size, grad_clip, momentum, X_test, Y_test, l2_reg, verbose, save)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'##### EPOCH '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' #####'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LState'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1948464976.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, optim, train_loader, grad_clip, l2_reg)\u001b[0m\n\u001b[1;32m     94\u001b[0m               \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m               \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m               \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m               \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":["if torch.cuda.device_count() > 1:\n","  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"],"metadata":{"execution":{"iopub.status.busy":"2023-04-11T12:56:27.746063Z","iopub.execute_input":"2023-04-11T12:56:27.747100Z","iopub.status.idle":"2023-04-11T12:56:27.753699Z","shell.execute_reply.started":"2023-04-11T12:56:27.747055Z","shell.execute_reply":"2023-04-11T12:56:27.752491Z"},"id":"qcBE63IrqNJO","outputId":"1774323b-f909-49f3-d259-a9b7100f5e92","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"}]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","best_state = deepcopy(seq_bc.losses['BState'])\n","seq_bc.model.load_state_dict(best_state)\n","print(np.min(seq_bc.losses['Test']))\n","\n","fig = make_subplots(rows=3, cols=1)\n","s = 0\n","fig.append_trace(go.Scatter(x=seq_bc.losses['Epoch'][s:], y=seq_bc.losses['Train'][s:],mode='lines',name='Train'), row=1, col=1)\n","fig.append_trace(go.Scatter(x=seq_bc.losses['Epoch'][s:], y=seq_bc.losses['Test'][s:],mode='lines',name='Test'), row=2, col=1)\n","fig.append_trace(go.Scatter(x=seq_bc.losses['Epoch'][s:], y=seq_bc.losses['LR'][s:],mode='lines',name='LR'), row=3, col=1)\n","fig.update_layout(height=1000, width=1500, title_text=\"Stacked Subplots\")\n","fig.show()"],"metadata":{"id":"DawdQHTYSv76","execution":{"iopub.status.busy":"2023-04-11T19:05:12.451065Z","iopub.execute_input":"2023-04-11T19:05:12.451437Z","iopub.status.idle":"2023-04-11T19:05:12.831026Z","shell.execute_reply.started":"2023-04-11T19:05:12.451406Z","shell.execute_reply":"2023-04-11T19:05:12.829942Z"},"trusted":true,"outputId":"34f62177-63b7-4454-8a3e-5f97da4698dc"},"execution_count":null,"outputs":[{"name":"stdout","text":"0.1459776452176273\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.18.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"88ec0161-b34d-4273-9244-3389bea4bf24\" class=\"plotly-graph-div\" style=\"height:1000px; width:1500px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"88ec0161-b34d-4273-9244-3389bea4bf24\")) {                    Plotly.newPlot(                        \"88ec0161-b34d-4273-9244-3389bea4bf24\",                        [{\"mode\":\"lines\",\"name\":\"Train\",\"x\":[0,1,2,3],\"y\":[0.2301355963565409,0.1402164114361629,0.11969724256759509,0.1035334189969115],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Test\",\"x\":[0,1,2,3],\"y\":[0.15782910123765467,0.1459776452176273,0.14809688719436526,0.1495963387325406],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines\",\"name\":\"LR\",\"x\":[0,1,2,3],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.7333333333333333,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.36666666666666664,0.6333333333333333]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,1.0]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.26666666666666666]},\"title\":{\"text\":\"Stacked Subplots\"},\"height\":1000,\"width\":1500},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('88ec0161-b34d-4273-9244-3389bea4bf24');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":["output = seq_bc.prdict(df_valid[text_column].tolist(), 2)\n","pred = np.round(np.asarray(output['pred']))\n","auc = roc_auc_score(df_valid[out_column].values, pred)\n","pd.DataFrame({'AUC': auc, 'ACC': accuracy_score(df_valid[out_column].values, np.round(pred)), \n","              'PRE': precision_score(df_valid[out_column].values, np.round(pred)), 'REC': recall_score(df_valid[out_column].values, np.round(pred)), \n","              'F1':f1_score(df_valid[out_column].values, np.round(pred))}, index=[0])"],"metadata":{"id":"jSTTd0YgSx87","execution":{"iopub.status.busy":"2023-04-11T19:07:31.085511Z","iopub.execute_input":"2023-04-11T19:07:31.085880Z","iopub.status.idle":"2023-04-11T19:09:42.074962Z","shell.execute_reply.started":"2023-04-11T19:07:31.085847Z","shell.execute_reply":"2023-04-11T19:09:42.073962Z"},"trusted":true,"outputId":"e51df962-a3df-4bba-c9ef-a43c912e1d0f"},"execution_count":null,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning:\n\nThe `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        AUC     ACC       PRE       REC        F1\n0  0.950352  0.9502  0.966366  0.933809  0.949809","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AUC</th>\n      <th>ACC</th>\n      <th>PRE</th>\n      <th>REC</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.950352</td>\n      <td>0.9502</td>\n      <td>0.966366</td>\n      <td>0.933809</td>\n      <td>0.949809</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["output = seq_bc.prdict(df_test[text_column].tolist(), 2)\n","pred = np.round(np.asarray(output['pred']))\n","auc = roc_auc_score(df_test[out_column].values, pred)\n","pd.DataFrame({'AUC': auc, 'ACC': accuracy_score(df_test[out_column].values, np.round(pred)), \n","              'PRE': precision_score(df_test[out_column].values, np.round(pred)), 'REC': recall_score(df_test[out_column].values, np.round(pred)), \n","              'F1':f1_score(df_test[out_column].values, np.round(pred))}, index=[0])"],"metadata":{"id":"-WkiZGg7Szga","execution":{"iopub.status.busy":"2023-04-11T19:10:53.071378Z","iopub.execute_input":"2023-04-11T19:10:53.071771Z","iopub.status.idle":"2023-04-11T19:13:04.298411Z","shell.execute_reply.started":"2023-04-11T19:10:53.071737Z","shell.execute_reply":"2023-04-11T19:13:04.297262Z"},"trusted":true,"outputId":"95955680-66f9-413a-fa6c-9bfd2c5f84c2"},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning:\n\nThe `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"        AUC     ACC      PRE       REC        F1\n0  0.955134  0.9552  0.96269  0.946393  0.954472","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AUC</th>\n      <th>ACC</th>\n      <th>PRE</th>\n      <th>REC</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.955134</td>\n      <td>0.9552</td>\n      <td>0.96269</td>\n      <td>0.946393</td>\n      <td>0.954472</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["confusion_matrix(df_test[out_column].values, pred)"],"metadata":{"id":"ScjmdjbuqNJR","execution":{"iopub.status.busy":"2023-04-11T19:13:33.648524Z","iopub.execute_input":"2023-04-11T19:13:33.648898Z","iopub.status.idle":"2023-04-11T19:13:33.660710Z","shell.execute_reply.started":"2023-04-11T19:13:33.648865Z","shell.execute_reply":"2023-04-11T19:13:33.659378Z"},"trusted":true,"outputId":"80da37b9-585f-4f6c-df29-e2f17f3286a2"},"execution_count":null,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[2428,   91],\n       [ 133, 2348]])"},"metadata":{}}]},{"cell_type":"code","source":["pd.concat((df_test.reset_index(drop=True), pd.DataFrame({'Pred': pred.ravel()})), axis=1)"],"metadata":{"id":"7loP5jYqqNJS","execution":{"iopub.status.busy":"2023-04-11T19:13:38.036473Z","iopub.execute_input":"2023-04-11T19:13:38.036836Z","iopub.status.idle":"2023-04-11T19:13:38.054955Z","shell.execute_reply.started":"2023-04-11T19:13:38.036801Z","shell.execute_reply":"2023-04-11T19:13:38.053491Z"},"trusted":true,"outputId":"7b72d005-20dc-4d00-8466-4c2a10996446"},"execution_count":null,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                 review  sentiment  Pred\n0     This movie was borderline in crude humor....I ...          0   0.0\n1     If you have seen Friends, the writing will fee...          1   1.0\n2     I was sadly disappointed by this film due to t...          1   1.0\n3     Broad enough for you? Wait till you see this h...          1   1.0\n4     Hitchcock was of the opinion that audiences ar...          0   1.0\n...                                                 ...        ...   ...\n4995  `Shadow Magic' recaptures the joy and amazemen...          0   0.0\n4996  I found this movie to be quite enjoyable and f...          0   0.0\n4997  Avoid this one! It is a terrible movie. So wha...          1   1.0\n4998  This production was quite a surprise for me. I...          0   0.0\n4999  This is a decent movie. Although little bit sh...          0   0.0\n\n[5000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>Pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>This movie was borderline in crude humor....I ...</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>If you have seen Friends, the writing will fee...</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I was sadly disappointed by this film due to t...</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Broad enough for you? Wait till you see this h...</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hitchcock was of the opinion that audiences ar...</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>`Shadow Magic' recaptures the joy and amazemen...</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>I found this movie to be quite enjoyable and f...</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>Avoid this one! It is a terrible movie. So wha...</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>This production was quite a surprise for me. I...</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>This is a decent movie. Although little bit sh...</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 3 columns</p>\n</div>"},"metadata":{}}]}]}